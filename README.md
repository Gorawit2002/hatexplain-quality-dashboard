# ğŸ¯ AI Data Labeling Quality Dashboard

An interactive dashboard for analyzing data labeling quality and inter-annotator agreement using the **HateXplain** dataset.

---

## ğŸ¯ Project Overview

Data labeling quality is critical for training reliable AI models. This project analyzes a real hate speech dataset to:

- Measure inter-annotator agreement using **Krippendorff's Alpha**
- Identify patterns in annotator disagreements
- Perform **Root Cause Analysis (RCA)** on labeling inconsistencies
- Provide actionable recommendations for guideline improvements

---

## ğŸ“ˆ Key Findings

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Krippendorff's Alpha | **0.46** | Below acceptable threshold (0.667) |
| Full Agreement Rate | **48.9%** | Less than half have unanimous agreement |
| Main Confusion | **offensive â†” hatespeech** | Boundary between these labels is unclear |
| Annotator Bias | **43% lenient** | Dataset likely under-labels hate speech |

---

## ğŸ› ï¸ Skills Demonstrated

| Skill | Application |
|-------|-------------|
| **Inter-Annotator Agreement** | Krippendorff's Alpha calculation and interpretation |
| **Quality Metrics & KPIs** | Dashboard tracking key quality indicators |
| **Root Cause Analysis** | Categorized disagreement patterns into 7 RCA categories |
| **Annotator Performance** | Bias detection (strict vs lenient annotators) |
| **Data Visualization** | Interactive Plotly charts for stakeholder communication |
| **Dashboard Development** | Multi-page Streamlit application |

---

## ğŸ“ Project Structure

```
hatexplain-quality-dashboard/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                    # Main Streamlit app
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Overview.py      # Quality metrics dashboard
â”‚   â”‚   â”œâ”€â”€ Disagreement_Explorer.py
â”‚   â”‚   â”œâ”€â”€ Annotator_Analysis.py
â”‚   â”‚   â””â”€â”€ RCA_Summary.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ data_loader.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ posts_analysis.csv
â”‚   â”œâ”€â”€ annotators_analysis.csv
â”‚   â”œâ”€â”€ summary_metrics.csv
â”‚   â””â”€â”€ disagreement_samples.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ HateXplain_Data_Exploration.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Run Locally

```bash
# Clone the repository
git clone https://github.com/yourusername/hatexplain-quality-dashboard.git
cd hatexplain-quality-dashboard

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run the dashboard
streamlit run app/app.py
```

### Deactivate venv (when done)

```bash
deactivate
```

---

## ğŸ“Š Dashboard Pages

### 1. ğŸ“Š Overview
Key metrics at a glance: Krippendorff's Alpha gauge, agreement distribution, label distribution, and annotator bias summary.

### 2. ğŸ” Disagreement Explorer
Interactive exploration of 10,303 samples where annotators disagreed. Filter by agreement type, label, and RCA category.

### 3. ğŸ‘¥ Annotator Analysis
Individual annotator performance: bias detection (strict/lenient/balanced), agreement rates, and quality leaderboard.

### 4. ğŸ¯ RCA Summary
Root cause analysis findings, proposed guideline updates, and priority review queue for edge cases.

---

## ğŸ“ Methodology

### Why Krippendorff's Alpha?

Simple percentage agreement doesn't account for chance. With 3 labels, random guessing yields ~33% agreement. Krippendorff's Alpha corrects for this.

**Thresholds (Krippendorff, 2004):**
- Î± â‰¥ 0.8: Reliable
- Î± â‰¥ 0.667: Acceptable  
- Î± < 0.667: Poor â† Our dataset (0.46)

### Annotator Bias Detection

```
Strictness Score = % hatespeech - % normal
```

| Score | Category | Percentage |
|-------|----------|------------|
| > 20 | Strict | 18% |
| -20 to 20 | Balanced | 39% |
| < -20 | Lenient | 43% |

### RCA Categories

| Category | % of Disagreements |
|----------|-------------------|
| Other/Unclear | 57.0% |
| Group References | 6.4% |
| Profanity | 2.4% |
| Racial Slurs | 2.3% |
| Gender/Sexuality | 2.2% |
| Religious Terms | 1.9% |
| Dehumanizing | 1.4% |

---

## ğŸ’¡ Key Recommendations

Based on the analysis:

1. **Clarify offensive vs hatespeech**: Hatespeech requires target group + negative intent
2. **Group references**: Mention alone â‰  hate speech (context matters)
3. **Slurs**: Default to hatespeech unless clearly reclaimed/quoted
4. **Calibration sessions**: Focus on the 43% lenient annotators

---

## ğŸ”§ Tech Stack

- **Python 3.9+**
- **Streamlit** - Interactive dashboard
- **Pandas** - Data manipulation
- **Plotly** - Interactive visualizations
- **Krippendorff** - Agreement calculation

---

## ğŸ“ Dataset

**HateXplain** (Mathew et al., 2020)

- 20,148 posts from Twitter and Gab
- 3 annotators per sample
- Labels: `normal`, `offensive`, `hatespeech`
- Includes rationales (highlighted words)

ğŸ“ Source: [HateXplain GitHub](https://github.com/hate-alert/HateXplain)

---

## ğŸ“„ License

MIT License - This project is for educational/portfolio purposes.

---

## ğŸ‘¤ Author

Built as a data quality analysis portfolio project demonstrating skills relevant to AI data labeling and annotation quality roles.